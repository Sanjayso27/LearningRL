{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6XNDa1NQUpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6b4310-1cd0-4c42-8d4e-e8027f8b124b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------\n",
            "Beginning Policy Iteration\n",
            "-------------------------\n",
            "[0.075 0.089 0.169 0.424 1.17  3.3  ]\n",
            "['R', 'R', 'R', 'R', 'R', 'R']\n",
            "\n",
            "-------------------------\n",
            "Beginning Value Iteration\n",
            "-------------------------\n",
            "[0.079 0.093 0.173 0.428 1.173 3.303]\n",
            "['R', 'R', 'R', 'R', 'R', 'R']\n"
          ]
        }
      ],
      "source": [
        "### MDP Value Iteration and Policy Iteration\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "def bellman_backup(state, action, R, T, gamma, V):\n",
        "    \"\"\"\n",
        "    Perform a single Bellman backup.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state: int\n",
        "    action: int\n",
        "    R: np.array (num_states, num_actions)\n",
        "    T: np.array (num_states, num_actions, num_states)\n",
        "    gamma: float\n",
        "    V: np.array (num_states)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    backup_val: float\n",
        "    \"\"\"\n",
        "    backup_val = 0.\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    backup_val = R[state,action]+gamma*np.dot(T[state,action],V)\n",
        "    ############################\n",
        "\n",
        "    return backup_val\n",
        "\n",
        "def policy_evaluation(policy, R, T, gamma, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Compute the value function induced by a given policy for the input MDP\n",
        "    Parameters\n",
        "    ----------\n",
        "    policy: np.array (num_states)\n",
        "    R: np.array (num_states, num_actions)\n",
        "    T: np.array (num_states, num_actions, num_states)\n",
        "    gamma: float\n",
        "    tol: float\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    value_function: np.array (num_states)\n",
        "    \"\"\"\n",
        "    num_states, num_actions = R.shape\n",
        "    value_function = np.zeros(num_states)\n",
        "\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    while True:\n",
        "      nxt_value_function=np.zeros(num_states)\n",
        "      stop=True\n",
        "      for state in range(num_states):\n",
        "        nxt_value_function[state] = R[state,policy[state]]+gamma*np.dot(T[state,policy[state]],value_function)\n",
        "        if(abs(nxt_value_function[state]-value_function[state])>tol):\n",
        "          stop=False\n",
        "      if(stop):\n",
        "        break\n",
        "      value_function=nxt_value_function\n",
        "\n",
        "\n",
        "    ############################\n",
        "    return value_function\n",
        "\n",
        "\n",
        "def policy_improvement(policy, R, T, V_policy, gamma):\n",
        "    \"\"\"\n",
        "    Given the value function induced by a given policy, perform policy improvement\n",
        "    Parameters\n",
        "    ----------\n",
        "    policy: np.array (num_states)\n",
        "    R: np.array (num_states, num_actions)\n",
        "    T: np.array (num_states, num_actions, num_states)\n",
        "    V_policy: np.array (num_states)\n",
        "    gamma: float\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    new_policy: np.array (num_states)\n",
        "    \"\"\"\n",
        "    num_states, num_actions = R.shape\n",
        "    new_policy = np.zeros(num_states, dtype=int)\n",
        "\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    for state in range(num_states):\n",
        "      mx=0.0\n",
        "      bst_action=0\n",
        "      for action in range(num_actions):\n",
        "        val=bellman_backup(state,action,R,T,gamma,V_policy)\n",
        "        if(val>mx):\n",
        "          bst_action=action\n",
        "          mx=val\n",
        "      new_policy[state]=bst_action\n",
        "    ############################\n",
        "    return new_policy\n",
        "\n",
        "\n",
        "def policy_iteration(R, T, gamma, tol=1e-3):\n",
        "    \"\"\"Runs policy iteration.\n",
        "\n",
        "    You should call the policy_evaluation() and policy_improvement() methods to\n",
        "    implement this method.\n",
        "    Parameters\n",
        "    ----------\n",
        "    R: np.array (num_states, num_actions)\n",
        "    T: np.array (num_states, num_actions, num_states)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    V_policy: np.array (num_states)\n",
        "    policy: np.array (num_states)\n",
        "    \"\"\"\n",
        "    num_states, num_actions = R.shape\n",
        "    V_policy = np.zeros(num_states)\n",
        "    policy = np.zeros(num_states, dtype=int)\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    while True:\n",
        "      V_upd=policy_evaluation(policy, R, T, gamma, tol)\n",
        "      stop=True\n",
        "      for state in range(num_states):\n",
        "        if(abs(V_upd[state]-V_policy[state])>tol):\n",
        "          stop=False\n",
        "      if(stop):\n",
        "        break;\n",
        "      V_policy=V_upd\n",
        "      policy=policy_improvement(policy, R, T, V_policy, gamma)\n",
        "\n",
        "    ############################\n",
        "    return V_policy, policy\n",
        "\n",
        "\n",
        "def value_iteration(R, T, gamma, tol=1e-3):\n",
        "    \"\"\"Runs value iteration.\n",
        "    Parameters\n",
        "    ----------\n",
        "    R: np.array (num_states, num_actions)\n",
        "    T: np.array (num_states, num_actions, num_states)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    value_function: np.array (num_states)\n",
        "    policy: np.array (num_states)\n",
        "    \"\"\"\n",
        "    num_states, num_actions = R.shape\n",
        "    value_function = np.zeros(num_states)\n",
        "    policy = np.zeros(num_states, dtype=int)\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    while True:\n",
        "      stop=True\n",
        "      nxt_value_function=np.zeros(num_states)\n",
        "      for state in range(num_states):\n",
        "        mx=0.0\n",
        "        bst_action=0\n",
        "        for action in range(num_actions):\n",
        "          val=bellman_backup(state, action, R, T, gamma, value_function)\n",
        "          if(val>mx):\n",
        "            mx=val\n",
        "            bst_action=action\n",
        "        if(abs(mx-value_function[state])>tol):\n",
        "          stop=False\n",
        "        nxt_value_function[state]=mx\n",
        "        policy[state]=bst_action\n",
        "      if(stop):\n",
        "        break\n",
        "      value_function=nxt_value_function\n",
        "    ############################\n",
        "    return value_function, policy\n",
        "\n",
        "\n",
        "# Edit below to run policy and value iteration on different configurations\n",
        "# You may change the parameters in the functions below\n",
        "SEED = 1234\n",
        "\n",
        "RIVER_CURRENT = 'STRONG'\n",
        "assert RIVER_CURRENT in ['WEAK', 'MEDIUM', 'STRONG']\n",
        "env = RiverSwim(RIVER_CURRENT, SEED)\n",
        "\n",
        "R, T = env.get_model()\n",
        "discount_factor = 0.94\n",
        "\n",
        "print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Policy Iteration\\n\" + \"-\" * 25)\n",
        "\n",
        "V_pi, policy_pi = policy_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
        "print(V_pi)\n",
        "print([['L', 'R'][a] for a in policy_pi])\n",
        "\n",
        "print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Value Iteration\\n\" + \"-\" * 25)\n",
        "\n",
        "V_vi, policy_vi = value_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
        "print(V_vi)\n",
        "print([['L', 'R'][a] for a in policy_vi])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.67 -> largest discount factor such that an optimal agent starting\n",
        "in the initial far-left state does not swim up the river for weak current.  \n",
        "0.77 -> largest discount factor such that an optimal agent starting\n",
        "in the initial far-left state does not swim up the river for medium current.  \n",
        "0.93 -> largest discount factor such that an optimal agent starting\n",
        "in the initial far-left state does not swim up the river for strong current.  "
      ],
      "metadata": {
        "id": "g8m5h4rwR3eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self, current, seed=1234):\n",
        "        self.num_states = 6\n",
        "        self.num_actions = 2  # O <=> LEFT, 1 <=> RIGHT\n",
        "\n",
        "        # Larger current makes it harder to swim up the river\n",
        "        self.currents = ['WEAK', 'MEDIUM', 'STRONG']\n",
        "        assert current in self.currents\n",
        "        self.current = self.currents.index(current) + 1\n",
        "        assert self.current in [1, 2, 3]\n",
        "\n",
        "        # Configure reward function\n",
        "        R = np.zeros((self.num_states, self.num_actions))\n",
        "        R[0, 0] = 0.005\n",
        "        R[5, 1] = 1.\n",
        "\n",
        "        # Configure transition function\n",
        "        T = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "\n",
        "        # Encode initial and rewarding state transitions\n",
        "        T[0, 0, 0] = 1.\n",
        "        T[0, 1, 0] = 0.6\n",
        "        T[0, 1, 1] = 0.4\n",
        "\n",
        "        T[5, 1, 5] = 0.6\n",
        "        T[5, 1, 4] = 0.4\n",
        "        T[5, 0, 4] = 1.\n",
        "\n",
        "        # Encode intermediate state transitions\n",
        "        for s in range(1, self.num_states - 1):\n",
        "            left, right = 0, 1\n",
        "\n",
        "            # Going left always succeeds\n",
        "            T[s, left, s - 1] = 1.\n",
        "\n",
        "            # Going right sometimes succeeds\n",
        "            T[s, right, s] = 0.6\n",
        "            T[s, right, s - 1] = 0.09 * self.current\n",
        "            T[s, right, s + 1] = 0.4 - T[s, right, s - 1]\n",
        "            assert np.isclose(np.sum(T[s, right]), 1.)\n",
        "\n",
        "        self.R = np.array(R)\n",
        "        self.T = np.array(T)\n",
        "\n",
        "        # Agent always starts at the opposite end of the river\n",
        "        self.init_state = 0\n",
        "        self.curr_state = self.init_state\n",
        "\n",
        "        self.seed = seed\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "    def get_model(self):\n",
        "        return copy.deepcopy(self.R), copy.deepcopy(self.T)\n",
        "\n",
        "    def reset(self):\n",
        "        return self.init_state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = self.R[self.curr_state, action]\n",
        "        next_state = np.random.choice(range(self.num_states), p=self.T[self.curr_state, action])\n",
        "        self.curr_state = next_state\n",
        "        return reward, next_state"
      ],
      "metadata": {
        "id": "4smbqvh1QYGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}