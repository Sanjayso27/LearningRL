{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPtJskehacY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5f2a72-e115-47bd-9206-fe929ccc95f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Stanford-CS-234-RL-2022'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 121 (delta 37), reused 81 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (121/121), 223.33 KiB | 3.19 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NickKaparinos/Stanford-CS-234-RL-2022.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def build_mlp(input_size, output_size, n_layers, size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_size: int, the dimension of inputs to be given to the network\n",
        "        output_size: int, the dimension of the output\n",
        "        n_layers: int, the number of hidden layers of the network\n",
        "        size: int, the size of each hidden layer\n",
        "    Returns:\n",
        "        An instance of (a subclass of) nn.Module representing the network.\n",
        "\n",
        "    TODO:\n",
        "    Build a feed-forward network (multi-layer perceptron, or mlp) that maps\n",
        "    input_size-dimensional vectors to output_size-dimensional vectors.\n",
        "    It should have 'n_layers' hidden layers, each of 'size' units and followed\n",
        "    by a ReLU nonlinearity. The final layer should be linear (no ReLU).\n",
        "\n",
        "    \"nn.Linear\" and \"nn.Sequential\" may be helpful.\n",
        "    \"\"\"\n",
        "    #######################################################\n",
        "    #########   YOUR CODE HERE - 7-15 lines.   ############\n",
        "    layers = [nn.Linear(input_size, size), nn.ReLU()]\n",
        "    for i in range(n_layers - 2):\n",
        "        layers.extend([nn.Linear(size, size), nn.ReLU()])\n",
        "    layers.append(nn.Linear(size, output_size))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "    #######################################################\n",
        "    #########          END YOUR CODE.          ############\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def np2torch(x, cast_double_to_float=True):\n",
        "    \"\"\"\n",
        "    Utility function that accepts a numpy array and does the following:\n",
        "        1. Convert to torch tensor\n",
        "        2. Move it to the GPU (if CUDA is available)\n",
        "        3. Optionally casts float64 to float32 (torch is picky about types)\n",
        "    \"\"\"\n",
        "    x = torch.from_numpy(x).to(device)\n",
        "    if cast_double_to_float and x.dtype is torch.float64:\n",
        "        x = x.float()\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "nSPdoKRU9ySI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import os\n",
        "from general import get_logger, Progbar, export_plot\n",
        "from baseline_network import BaselineNetwork\n",
        "from network_utils import build_mlp, device, np2torch\n",
        "from policy import CategoricalPolicy, GaussianPolicy\n",
        "\n",
        "\n",
        "class PolicyGradient(object):\n",
        "    \"\"\"\n",
        "    Class for implementing a policy gradient algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, config, seed, logger=None):\n",
        "        \"\"\"\n",
        "        Initialize Policy Gradient Class\n",
        "\n",
        "        Args:\n",
        "                env: an OpenAI Gym environment\n",
        "                config: class with hyperparameters\n",
        "                logger: logger instance from the logging module\n",
        "\n",
        "        You do not need to implement anything in this function. However,\n",
        "        you will need to use self.discrete, self.observation_dim,\n",
        "        self.action_dim, and self.lr in other methods.\n",
        "        \"\"\"\n",
        "        # directory for training outputs\n",
        "        if not os.path.exists(config.output_path):\n",
        "            os.makedirs(config.output_path)\n",
        "\n",
        "        # store hyperparameters\n",
        "        self.config = config\n",
        "        self.seed = seed\n",
        "\n",
        "        self.logger = logger\n",
        "        if logger is None:\n",
        "            self.logger = get_logger(config.log_path)\n",
        "        self.env = env\n",
        "        self.env.seed(self.seed)\n",
        "\n",
        "        # discrete vs continuous action space\n",
        "        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "        self.observation_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = (\n",
        "            self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
        "        )\n",
        "\n",
        "        self.lr = self.config.learning_rate\n",
        "\n",
        "        self.init_policy()\n",
        "\n",
        "        if config.use_baseline:\n",
        "            self.baseline_network = BaselineNetwork(env, config)\n",
        "\n",
        "    def init_policy(self):\n",
        "        \"\"\"\n",
        "        Please do the following:\n",
        "        1. Create a network using build_mlp. It should map vectors of size\n",
        "           self.observation_dim to vectors of size self.action_dim, and use\n",
        "           the number of layers and layer size from self.config\n",
        "        2. If self.discrete = True (meaning that the actions are discrete, i.e.\n",
        "           from the set {0, 1, ..., N-1} where N is the number of actions),\n",
        "           instantiate a CategoricalPolicy.\n",
        "           If self.discrete = False (meaning that the actions are continuous,\n",
        "           i.e. elements of R^d where d is the dimension), instantiate a\n",
        "           GaussianPolicy. Either way, assign the policy to self.policy\n",
        "        3. Create an optimizer for the policy, with learning rate self.lr\n",
        "           Note that the policy is an instance of (a subclass of) nn.Module, so\n",
        "           you can call the parameters() method to get its parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        network = build_mlp(self.observation_dim, self.action_dim, self.config.n_layers, self.config.layer_size)\n",
        "        if self.discrete:\n",
        "            self.policy = CategoricalPolicy(network).to(device)\n",
        "        else:\n",
        "            self.policy = GaussianPolicy(network, self.action_dim).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "    def init_averages(self):\n",
        "        \"\"\"\n",
        "        You don't have to change or use anything here.\n",
        "        \"\"\"\n",
        "        self.avg_reward = 0.0\n",
        "        self.max_reward = 0.0\n",
        "        self.std_reward = 0.0\n",
        "        self.eval_reward = 0.0\n",
        "\n",
        "    def update_averages(self, rewards, scores_eval):\n",
        "        \"\"\"\n",
        "        Update the averages.\n",
        "        You don't have to change or use anything here.\n",
        "\n",
        "        Args:\n",
        "            rewards: deque\n",
        "            scores_eval: list\n",
        "        \"\"\"\n",
        "        self.avg_reward = np.mean(rewards)\n",
        "        self.max_reward = np.max(rewards)\n",
        "        self.std_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
        "\n",
        "        if len(scores_eval) > 0:\n",
        "            self.eval_reward = scores_eval[-1]\n",
        "\n",
        "    def record_summary(self, t):\n",
        "        pass\n",
        "\n",
        "    def sample_path(self, env, num_episodes=None):\n",
        "        \"\"\"\n",
        "        Sample paths (trajectories) from the environment.\n",
        "\n",
        "        Args:\n",
        "            num_episodes: the number of episodes to be sampled\n",
        "                if none, sample one batch (size indicated by config file)\n",
        "            env: open AI Gym envinronment\n",
        "\n",
        "        Returns:\n",
        "            paths: a list of paths. Each path in paths is a dictionary with\n",
        "                path[\"observation\"] a numpy array of ordered observations in the path\n",
        "                path[\"actions\"] a numpy array of the corresponding actions in the path\n",
        "                path[\"reward\"] a numpy array of the corresponding rewards in the path\n",
        "            total_rewards: the sum of all rewards encountered during this \"path\"\n",
        "\n",
        "        You do not have to implement anything in this function, but you will need to\n",
        "        understand what it returns, and it is worthwhile to look over the code\n",
        "        just so you understand how we are taking actions in the environment\n",
        "        and generating batches to train on.\n",
        "        \"\"\"\n",
        "        episode = 0\n",
        "        episode_rewards = []\n",
        "        paths = []\n",
        "        t = 0\n",
        "\n",
        "        while num_episodes or t < self.config.batch_size:\n",
        "            state = env.reset()\n",
        "            states, actions, rewards = [], [], []\n",
        "            episode_reward = 0\n",
        "\n",
        "            for step in range(self.config.max_ep_len):\n",
        "                states.append(state)\n",
        "                action = self.policy.act(states[-1][None])[0]\n",
        "                state, reward, done, info = env.step(action)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                episode_reward += reward\n",
        "                t += 1\n",
        "                if done or step == self.config.max_ep_len - 1:\n",
        "                    episode_rewards.append(episode_reward)\n",
        "                    break\n",
        "                if (not num_episodes) and t == self.config.batch_size:\n",
        "                    break\n",
        "\n",
        "            path = {\n",
        "                \"observation\": np.array(states),\n",
        "                \"reward\": np.array(rewards),\n",
        "                \"action\": np.array(actions),\n",
        "            }\n",
        "            paths.append(path)\n",
        "            episode += 1\n",
        "            if num_episodes and episode >= num_episodes:\n",
        "                break\n",
        "\n",
        "        return paths, episode_rewards\n",
        "\n",
        "    def get_returns(self, paths):\n",
        "        \"\"\"\n",
        "        Calculate the returns G_t for each timestep\n",
        "\n",
        "        Args:\n",
        "            paths: recorded sample paths. See sample_path() for details.\n",
        "\n",
        "        Return:\n",
        "            returns: return G_t for each timestep\n",
        "\n",
        "        After acting in the environment, we record the observations, actions, and\n",
        "        rewards. To get the advantages that we need for the policy update, we have\n",
        "        to convert the rewards into returns, G_t, which are themselves an estimate\n",
        "        of Q^π (s_t, a_t):\n",
        "\n",
        "           G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ... + γ^{T-t} r_T\n",
        "\n",
        "        where T is the last timestep of the episode.\n",
        "\n",
        "        Note that here we are creating a list of returns for each path\n",
        "\n",
        "        TODO: compute and return G_t for each timestep. Use self.config.gamma.\n",
        "        \"\"\"\n",
        "\n",
        "        all_returns = []\n",
        "        for path in paths:\n",
        "            rewards = path[\"reward\"]\n",
        "\n",
        "            gamma = self.config.gamma\n",
        "            previous_returns = None\n",
        "            returns = []\n",
        "            for i in range(len(rewards)):\n",
        "                if previous_returns is None:\n",
        "                    temp_returns = 0\n",
        "                    for j in range(len(rewards)):\n",
        "                        temp_returns += gamma ** i * rewards[j]\n",
        "                else:\n",
        "                    temp_returns = (previous_returns - rewards[i - 1]) / gamma  # noqa\n",
        "                returns.append(temp_returns)\n",
        "                previous_returns = temp_returns\n",
        "\n",
        "            all_returns.append(returns)\n",
        "        returns = np.concatenate(all_returns)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def normalize_advantage(self, advantages):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            advantages: np.array of shape [batch size]\n",
        "        Returns:\n",
        "            normalized_advantages: np.array of shape [batch size]\n",
        "\n",
        "        TODO:\n",
        "        Normalize the advantages so that they have a mean of 0 and standard\n",
        "        deviation of 1. Put the result in a variable called\n",
        "        normalized_advantages (which will be returned).\n",
        "\n",
        "        Note:\n",
        "        This function is called only if self.config.normalize_advantage is True.\n",
        "        \"\"\"\n",
        "        normalized_advantages = (advantages - np.mean(advantages)) / np.std(advantages)\n",
        "        return normalized_advantages\n",
        "\n",
        "    def calculate_advantage(self, returns, observations):\n",
        "        \"\"\"\n",
        "        Calculates the advantage for each of the observations\n",
        "        Args:\n",
        "            returns: np.array of shape [batch size]\n",
        "            observations: np.array of shape [batch size, dim(observation space)]\n",
        "        Returns:\n",
        "            advantages: np.array of shape [batch size]\n",
        "        \"\"\"\n",
        "        if self.config.use_baseline:\n",
        "            # override the behavior of advantage by subtracting baseline\n",
        "            advantages = self.baseline_network.calculate_advantage(\n",
        "                returns, observations\n",
        "            )\n",
        "        else:\n",
        "            advantages = returns\n",
        "\n",
        "        if self.config.normalize_advantage:\n",
        "            advantages = self.normalize_advantage(advantages)\n",
        "\n",
        "        return advantages\n",
        "\n",
        "    def update_policy(self, observations, actions, advantages):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            observations: np.array of shape [batch size, dim(observation space)]\n",
        "            actions: np.array of shape\n",
        "                [batch size, dim(action space)] if continuous\n",
        "                [batch size] (and integer type) if discrete\n",
        "            advantages: np.array of shape [batch size]\n",
        "\n",
        "        Perform one update on the policy using the provided data.\n",
        "        To compute the loss, you will need the log probabilities of the actions\n",
        "        given the observations. Note that the policy's action_distribution\n",
        "        method returns an instance of a subclass of\n",
        "        torch.distributions.Distribution, and that object can be used to\n",
        "        compute log probabilities.\n",
        "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
        "\n",
        "        Note:\n",
        "        PyTorch optimizers will try to minimize the loss you compute, but you\n",
        "        want to maximize the policy's performance.\n",
        "        \"\"\"\n",
        "        observations = np2torch(observations)\n",
        "        actions = np2torch(actions)\n",
        "        advantages = np2torch(advantages)\n",
        "\n",
        "        distribution = self.policy.action_distribution(observations)\n",
        "        loss = -distribution.log_prob(actions) * advantages\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.sum().backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Performs training\n",
        "\n",
        "        You do not have to change or use anything here, but take a look\n",
        "        to see how all the code you've written fits together!\n",
        "        \"\"\"\n",
        "        last_record = 0\n",
        "\n",
        "        self.init_averages()\n",
        "        all_total_rewards = (\n",
        "            []\n",
        "        )  # the returns of all episodes samples for training purposes\n",
        "        averaged_total_rewards = []  # the returns for each iteration\n",
        "\n",
        "        for t in range(self.config.num_batches):\n",
        "\n",
        "            # collect a minibatch of samples\n",
        "            paths, total_rewards = self.sample_path(self.env)\n",
        "            all_total_rewards.extend(total_rewards)\n",
        "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
        "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
        "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
        "            # compute Q-val estimates (discounted future returns) for each time step\n",
        "            returns = self.get_returns(paths)\n",
        "\n",
        "            # advantage will depend on the baseline implementation\n",
        "            advantages = self.calculate_advantage(returns, observations)\n",
        "\n",
        "            # run training operations\n",
        "            if self.config.use_baseline:\n",
        "                self.baseline_network.update_baseline(returns, observations)\n",
        "            self.update_policy(observations, actions, advantages)\n",
        "\n",
        "            # logging\n",
        "            if t % self.config.summary_freq == 0:\n",
        "                self.update_averages(total_rewards, all_total_rewards)\n",
        "                self.record_summary(t)\n",
        "\n",
        "            # compute reward statistics for this batch and log\n",
        "            avg_reward = np.mean(total_rewards)\n",
        "            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))\n",
        "            msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(\n",
        "                avg_reward, sigma_reward\n",
        "            )\n",
        "            averaged_total_rewards.append(avg_reward)\n",
        "            self.logger.info(msg)\n",
        "\n",
        "            if self.config.record and (last_record > self.config.record_freq):\n",
        "                self.logger.info(\"Recording...\")\n",
        "                last_record = 0\n",
        "                self.record()\n",
        "\n",
        "        self.logger.info(\"- Training done.\")\n",
        "        np.save(self.config.scores_output, averaged_total_rewards)\n",
        "        export_plot(\n",
        "            averaged_total_rewards,\n",
        "            \"Score\",\n",
        "            self.config.env_name,\n",
        "            self.config.plot_output,\n",
        "        )\n",
        "\n",
        "    def evaluate(self, env=None, num_episodes=1):\n",
        "        \"\"\"\n",
        "        Evaluates the return for num_episodes episodes.\n",
        "        Not used right now, all evaluation statistics are computed during training\n",
        "        episodes.\n",
        "        \"\"\"\n",
        "        if env == None:\n",
        "            env = self.env\n",
        "        paths, rewards = self.sample_path(env, num_episodes)\n",
        "        avg_reward = np.mean(rewards)\n",
        "        sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
        "        msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
        "        self.logger.info(msg)\n",
        "        return avg_reward\n",
        "\n",
        "    def record(self):\n",
        "        \"\"\"\n",
        "        Recreate an env and record a video for one episode\n",
        "        \"\"\"\n",
        "        env = gym.make(self.config.env_name)\n",
        "        env.seed(self.seed)\n",
        "        env = gym.wrappers.Monitor(\n",
        "            env, self.config.record_path, video_callable=lambda x: True, resume=True\n",
        "        )\n",
        "        self.evaluate(env, 1)\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Apply procedures of training for a PG.\n",
        "        \"\"\"\n",
        "        # record one game at the beginning\n",
        "        if self.config.record:\n",
        "            self.record()\n",
        "        # model\n",
        "        self.train()\n",
        "        # record one game at the end\n",
        "        if self.config.record:\n",
        "            self.record()\n"
      ],
      "metadata": {
        "id": "oG5bXfyx_-A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from network_utils import build_mlp, device, np2torch\n",
        "\n",
        "\n",
        "class BaselineNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Class for implementing Baseline network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, config):\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "        Create self.network using build_mlp, and create self.optimizer to\n",
        "        optimize its parameters.\n",
        "        You should find some values in the config, such as the number of layers,\n",
        "        the size of the layers, and the learning rate.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.env = env\n",
        "        self.baseline = None\n",
        "        self.lr = self.config.learning_rate\n",
        "\n",
        "\n",
        "        self.network = build_mlp(env.observation_space.shape[0], 1, self.config.n_layers, self.config.layer_size).cuda()\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "    def forward(self, observations):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            observations: torch.Tensor of shape [batch size, dim(observation space)]\n",
        "        Returns:\n",
        "            output: torch.Tensor of shape [batch size]\n",
        "\n",
        "        TODO:\n",
        "        Run the network forward and then squeeze the result so that it's\n",
        "        1-dimensional. Put the squeezed result in a variable called \"output\"\n",
        "        (which will be returned).\n",
        "\n",
        "        Note:\n",
        "        A nn.Module's forward method will be invoked if you\n",
        "        call it like a function, e.g. self(x) will call self.forward(x).\n",
        "        When implementing other methods, you should use this instead of\n",
        "        directly referencing the network (so that the shape is correct).\n",
        "        \"\"\"\n",
        "\n",
        "        output = self.network(observations).squeeze()\n",
        "\n",
        "        assert output.ndim == 1\n",
        "        return output\n",
        "\n",
        "    def calculate_advantage(self, returns, observations):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            returns: np.array of shape [batch size]\n",
        "                all discounted future returns for each step\n",
        "            observations: np.array of shape [batch size, dim(observation space)]\n",
        "        Returns:\n",
        "            advantages: np.array of shape [batch size]\n",
        "\n",
        "        TODO:\n",
        "        Evaluate the baseline and use the result to compute the advantages.\n",
        "        Put the advantages in a variable called \"advantages\" (which will be\n",
        "        returned).\n",
        "\n",
        "        Note:\n",
        "        The arguments and return value are numpy arrays. The np2torch function\n",
        "        converts numpy arrays to torch tensors. You will have to convert the\n",
        "        network output back to numpy, which can be done via the numpy() method.\n",
        "        \"\"\"\n",
        "        observations = np2torch(observations)\n",
        "\n",
        "        advantages = self(observations).cpu().detach().numpy() - returns\n",
        "\n",
        "        return advantages\n",
        "\n",
        "    def update_baseline(self, returns, observations):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            returns: np.array of shape [batch size], containing all discounted\n",
        "                future returns for each step\n",
        "            observations: np.array of shape [batch size, dim(observation space)]\n",
        "\n",
        "        TODO:\n",
        "        Compute the loss (MSE), backpropagate, and step self.optimizer.\n",
        "        You may (though not necessary) find it useful to do perform these steps\n",
        "        more than one once, since this method is only called once per policy update.\n",
        "        If you want to use mini-batch SGD, we have provided a helper function\n",
        "        called batch_iterator (implemented in general.py).\n",
        "        \"\"\"\n",
        "        returns = np2torch(returns)\n",
        "        observations = np2torch(observations)\n",
        "\n",
        "        loss = torch.nn.MSELoss()\n",
        "        output = loss(self(observations), returns)\n",
        "        self.optimizer.zero_grad()\n",
        "        output.backward()\n",
        "        self.optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "JEIir0bBACZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}